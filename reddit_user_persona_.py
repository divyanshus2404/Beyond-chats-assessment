# -*- coding: utf-8 -*-
"""Reddit User Persona .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lvUlISfDU79e04TUYgdeVvRPSuaXGdve
"""

pip install praw openai python-dotenv

pip install praw transformers

import praw
from transformers import pipeline
import re

# 1. Reddit API credentials (DO NOT share these publicly in real-world apps)
reddit = praw.Reddit(
    client_id='52muhPiuVCj-YGgHc6bdPw',
    client_secret='yeBwaI7USL35fTc7mAVFeRUCIU4Rjw',
    user_agent='PersonaAnalyzer by u/Temporary_Camera3341'
)

# 2. Extract username from Reddit profile URL
def extract_username(profile_url):
    match = re.search(r'reddit\.com\/user\/([A-Za-z0-9_-]+)', profile_url)
    return match.group(1) if match else None

# 3. Fetch posts and comments from Reddit user
def fetch_user_content(username, limit=50):
    user = reddit.redditor(username)
    content = []

    try:
        for comment in user.comments.new(limit=limit):
            content.append(comment.body)

        for submission in user.submissions.new(limit=limit):
            content.append(submission.title + " " + (submission.selftext or ""))

    except Exception as e:
        print(f"Error fetching data: {e}")

    return content

# 4. Analyze with HuggingFace transformers
def analyze_persona(texts):
    sentiment_analyzer = pipeline("sentiment-analysis")
    classifier = pipeline("zero-shot-classification")

    joined_text = " ".join(texts[:30])[:2000]  # Trim long input
    topics = [
        "technology", "politics", "sports", "relationships", "finance",
        "gaming", "science", "health", "spirituality", "memes", "movies", "education"
    ]

    sentiment_result = sentiment_analyzer(joined_text[:1000])
    topic_result = classifier(joined_text, candidate_labels=topics)

    return {
        "sentiment": sentiment_result,
        "top_interests": topic_result["labels"][:3],
        "confidence_scores": topic_result["scores"][:3]
    }

# 5. Main function to run everything
def get_persona(profile_url):
    username = extract_username(profile_url)
    if not username:
        return "‚ùå Invalid Reddit profile URL."

    print(f"\nüîç Analyzing Reddit user: u/{username}...")

    texts = fetch_user_content(username)
    if not texts:
        return "‚ö†Ô∏è No content found for this user."

    result = analyze_persona(texts)
    persona_summary = f"""
üß† Persona Summary for u/{username}
-------------------------------------
Top Sentiment: {result['sentiment'][0]['label']} ({result['sentiment'][0]['score']:.2f})
Top Interests:
1. {result['top_interests'][0]} ({result['confidence_scores'][0]:.2f})
2. {result['top_interests'][1]} ({result['confidence_scores'][1]:.2f})
3. {result['top_interests'][2]} ({result['confidence_scores'][2]:.2f})
"""
    return persona_summary

# 6. Run for your client: u/kojied
if __name__ == "__main__":
    profile_url = "https://www.reddit.com/user/kojied/"
    persona = get_persona(profile_url)
    print(persona)

import praw
from transformers import pipeline
import re

# 1. Reddit API credentials (Your own from Reddit Developer Console)
reddit = praw.Reddit(
    client_id='52muhPiuVCj-YGgHc6bdPw',
    client_secret='yeBwaI7USL35fTc7mAVFeRUCIU4Rjw',
    user_agent='PersonaAnalyzer by u/Temporary_Camera3341'
)

# 2. Extract username from Reddit profile URL
def extract_username(profile_url):
    match = re.search(r'reddit\.com\/user\/([A-Za-z0-9_-]+)', profile_url)
    return match.group(1) if match else None

# 3. Fetch posts and comments from Reddit user
def fetch_user_content(username, limit=50):
    user = reddit.redditor(username)
    content = []

    try:
        for comment in user.comments.new(limit=limit):
            content.append(comment.body)

        for submission in user.submissions.new(limit=limit):
            content.append(submission.title + " " + (submission.selftext or ""))

    except Exception as e:
        print(f"Error fetching data: {e}")

    return content

# 4. Analyze text using HuggingFace transformers
def analyze_persona(texts):
    sentiment_analyzer = pipeline("sentiment-analysis")
    classifier = pipeline("zero-shot-classification")

    joined_text = " ".join(texts[:30])[:2000]  # Trim to model limits
    topics = [
        "technology", "politics", "sports", "relationships", "finance",
        "gaming", "science", "health", "spirituality", "memes", "movies", "education"
    ]

    sentiment_result = sentiment_analyzer(joined_text[:1000])
    topic_result = classifier(joined_text, candidate_labels=topics)

    return {
        "sentiment": sentiment_result,
        "top_interests": topic_result["labels"][:3],
        "confidence_scores": topic_result["scores"][:3]
    }

# 5. Main function to run everything
def get_persona(profile_url):
    username = extract_username(profile_url)
    if not username:
        return "‚ùå Invalid Reddit profile URL."

    print(f"\nüîç Analyzing Reddit user: u/{username}...")

    texts = fetch_user_content(username)
    if not texts:
        return "‚ö†Ô∏è No content found for this user."

    result = analyze_persona(texts)
    persona_summary = f"""
üß† Persona Summary for u/{username}
-------------------------------------
Top Sentiment: {result['sentiment'][0]['label']} ({result['sentiment'][0]['score']:.2f})
Top Interests:
1. {result['top_interests'][0]} ({result['confidence_scores'][0]:.2f})
2. {result['top_interests'][1]} ({result['confidence_scores'][1]:.2f})
3. {result['top_interests'][2]} ({result['confidence_scores'][2]:.2f})
"""
    return persona_summary

# 6. Run for user: u/Hungry-Move-6603
if __name__ == "__main__":
    profile_url = "https://www.reddit.com/user/Hungry-Move-6603/"
    persona = get_persona(profile_url)
    print(persona)

pip install praw transformers

import praw
from transformers import pipeline
import re
import os

# ‚úÖ Reddit API Setup
reddit = praw.Reddit(
    client_id='52muhPiuVCj-YGgHc6bdPw',
    client_secret='yeBwaI7USL35fTc7mAVFeRUCIU4Rjw',
    user_agent='PersonaAnalyzer by u/Temporary_Camera3341'
)

# ‚úÖ Extract Reddit username from URL
def extract_username(profile_url):
    match = re.search(r'reddit\.com\/user\/([A-Za-z0-9_-]+)', profile_url)
    return match.group(1) if match else None

# ‚úÖ Fetch posts and comments
def fetch_user_content(username, limit=50):
    user = reddit.redditor(username)
    content = []

    try:
        for comment in user.comments.new(limit=limit):
            content.append(comment.body)
        for submission in user.submissions.new(limit=limit):
            content.append(submission.title + " " + (submission.selftext or ""))
    except Exception as e:
        print(f"Error fetching data for u/{username}: {e}")

    return content

# ‚úÖ Analyze content using Transformers
def analyze_persona(texts):
    sentiment_analyzer = pipeline("sentiment-analysis")
    classifier = pipeline("zero-shot-classification")

    joined_text = " ".join(texts[:30])[:2000]  # Trim to model input size
    topics = [
        "technology", "politics", "sports", "relationships", "finance",
        "gaming", "science", "health", "spirituality", "memes", "movies", "education"
    ]

    sentiment_result = sentiment_analyzer(joined_text[:1000])
    topic_result = classifier(joined_text, candidate_labels=topics)

    return {
        "sentiment": sentiment_result,
        "top_interests": topic_result["labels"][:3],
        "confidence_scores": topic_result["scores"][:3]
    }

# ‚úÖ Generate and Save Persona
def create_persona(profile_url):
    username = extract_username(profile_url)
    if not username:
        print(f"‚ùå Invalid profile URL: {profile_url}")
        return

    print(f"\nüîç Analyzing Reddit user: u/{username}...")

    texts = fetch_user_content(username)
    if not texts:
        print(f"‚ö†Ô∏è No content found for u/{username}")
        return

    result = analyze_persona(texts)

    persona_summary = f"""
üß† Persona Summary for u/{username}
-------------------------------------
Top Sentiment: {result['sentiment'][0]['label']} ({result['sentiment'][0]['score']:.2f})

Top Interests:
1. {result['top_interests'][0]} ({result['confidence_scores'][0]:.2f})
2. {result['top_interests'][1]} ({result['confidence_scores'][1]:.2f})
3. {result['top_interests'][2]} ({result['confidence_scores'][2]:.2f})

Text Sample:
{texts[0][:250]}...
"""

    print(persona_summary)

    # Save to text file
    with open(f"persona_{username}.txt", "w", encoding="utf-8") as f:
        f.write(persona_summary)
    print(f"‚úÖ Persona saved to: persona_{username}.txt")

# ‚úÖ Main Function ‚Äì Call multiple users
if __name__ == "__main__":
    reddit_profiles = [
        "https://www.reddit.com/user/kojied/",
        "https://www.reddit.com/user/Hungry-Move-6603/"
    ]

    for url in reddit_profiles:
        create_persona(url)

import praw
from transformers import pipeline
import re
import os

# -------------------------------
# ‚úÖ Reddit API Setup
# -------------------------------
reddit = praw.Reddit(
    client_id='52muhPiuVCj-YGgHc6bdPw',
    client_secret='yeBwaI7USL35fTc7mAVFeRUCIU4Rjw',
    user_agent='PersonaAnalyzer by u/Temporary_Camera3341'
)

# -------------------------------
# ‚úÖ Extract username from URL
# -------------------------------
def extract_username(profile_url):
    match = re.search(r'reddit\.com\/user\/([A-Za-z0-9_-]+)', profile_url)
    return match.group(1) if match else None

# -------------------------------
# ‚úÖ Fetch user content
# -------------------------------
def fetch_user_content(username, limit=50):
    user = reddit.redditor(username)
    content = []

    try:
        for comment in user.comments.new(limit=limit):
            content.append(comment.body)
        for submission in user.submissions.new(limit=limit):
            text = submission.title + " " + (submission.selftext or "")
            content.append(text)
    except Exception as e:
        print(f"Error fetching data for u/{username}: {e}")

    return content

# -------------------------------
# ‚úÖ Analyze sentiment and topics
# -------------------------------
def analyze_content(texts):
    sentiment_analyzer = pipeline("sentiment-analysis")
    classifier = pipeline("zero-shot-classification")

    joined_text = " ".join(texts[:30])[:2000]
    topics = [
        "technology", "politics", "sports", "relationships", "finance",
        "gaming", "science", "health", "spirituality", "memes", "movies", "education"
    ]

    sentiment = sentiment_analyzer(joined_text[:1000])
    topic_result = classifier(joined_text, candidate_labels=topics)

    return {
        "sentiment": sentiment,
        "top_interests": topic_result["labels"][:3],
        "confidence_scores": topic_result["scores"][:3]
    }

# -------------------------------
# ‚úÖ Infer Personality Traits
# -------------------------------
def estimate_personality(texts):
    joined = " ".join(texts[:50]).lower()

    return {
        "Introvert": 70 if joined.count(" i ") > joined.count(" we ") else 30,
        "Extrovert": 30 if joined.count(" i ") > joined.count(" we ") else 70,
        "Thinking": 70 if "logic" in joined or "fact" in joined else 30,
        "Feeling": 70 if "feel" in joined or "emotion" in joined else 30,
        "Judging": 70 if "should" in joined or "must" in joined else 30,
        "Perceiving": 70 if "maybe" in joined or "could" in joined else 30
    }

# -------------------------------
# ‚úÖ Trait Classification
# -------------------------------
def classify_traits(text):
    classifier = pipeline("zero-shot-classification")
    traits = ["Humorous", "Helpful", "Sarcastic", "Curious", "Supportive", "Logical", "Argumentative"]
    result = classifier(text, candidate_labels=traits)
    return result["labels"][:4]

# -------------------------------
# ‚úÖ Persona Dictionary Generator
# -------------------------------
def generate_persona(username, texts, analysis, personality, traits):
    return {
        "name": f"u/{username}",
        "tier": "Active Redditor",
        "archetype": "The Explorer" if "Curious" in traits else "The Analyst",
        "traits": traits,
        "motivations": {
            interest: round(score * 100)
            for interest, score in zip(analysis["top_interests"], analysis["confidence_scores"])
        },
        "personality": personality,
        "habits": [
            "Posts frequently in niche subreddits.",
            "Engages with others through comments.",
            "Mixes opinion with helpfulness."
        ],
        "frustrations": [
            "Low-effort content or spam.",
            "Overgeneralized takes or poor reasoning."
        ],
        "goals": [
            "To connect with thoughtful users.",
            "To explore new ideas and perspectives."
        ],
        "quote": texts[0][:100] + "...",
        "sentiment": analysis["sentiment"][0]["label"]
    }

# -------------------------------
# ‚úÖ Render Persona as Text
# -------------------------------
def render_persona_text(persona):
    lines = [f"üß† Persona Summary for {persona['name']}\n" + "-" * 40]
    lines.append(f"Tier: {persona['tier']}")
    lines.append(f"Archetype: {persona['archetype']}")
    lines.append(f"Top Traits: {', '.join(persona['traits'])}")
    lines.append(f"Sentiment: {persona['sentiment']}")

    lines.append("\nMotivations:")
    for k, v in persona["motivations"].items():
        lines.append(f" - {k}: {v}/100")

    lines.append("\nPersonality:")
    for k, v in persona["personality"].items():
        lines.append(f" - {k}: {v}/100")

    lines.append("\nHabits:")
    lines.extend([f" - {h}" for h in persona["habits"]])

    lines.append("\nFrustrations:")
    lines.extend([f" - {f}" for f in persona["frustrations"]])

    lines.append("\nGoals:")
    lines.extend([f" - {g}" for g in persona["goals"]])

    lines.append(f"\nQuote:\n\"{persona['quote']}\"")
    return "\n".join(lines)

# -------------------------------
# ‚úÖ Full Pipeline
# -------------------------------
def create_persona(profile_url):
    username = extract_username(profile_url)
    if not username:
        print(f"‚ùå Invalid profile URL: {profile_url}")
        return

    print(f"\nüîç Analyzing Reddit user: u/{username}...")

    texts = fetch_user_content(username)
    if not texts:
        print(f"‚ö†Ô∏è No content found for u/{username}")
        return

    analysis = analyze_content(texts)
    personality = estimate_personality(texts)
    traits = classify_traits(" ".join(texts[:10]))

    persona = generate_persona(username, texts, analysis, personality, traits)
    summary_text = render_persona_text(persona)

    # Save to file
    file_path = f"persona_{username}.txt"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(summary_text)

    print(summary_text)
    print(f"\n‚úÖ Persona saved to: {file_path}")

# -------------------------------
# ‚úÖ Main
# -------------------------------
if __name__ == "__main__":
    reddit_profiles = [
        "https://www.reddit.com/user/Hungry-Move-6603/"
    ]
    for url in reddit_profiles:
        create_persona(url)

import praw
from transformers import pipeline
import re

# ‚úÖ Reddit API Setup
reddit = praw.Reddit(
    client_id='52muhPiuVCj-YGgHc6bdPw',
    client_secret='yeBwaI7USL35fTc7mAVFeRUCIU4Rjw',
    user_agent='PersonaAnalyzer by u/Temporary_Camera3341'
)

def extract_username(profile_url):
    match = re.search(r'reddit\.com\/user\/([A-Za-z0-9_-]+)', profile_url)
    return match.group(1) if match else None

def fetch_user_content(username, limit=50):
    user = reddit.redditor(username)
    content = []

    try:
        for comment in user.comments.new(limit=limit):
            content.append(comment.body)
        for submission in user.submissions.new(limit=limit):
            text = submission.title + " " + (submission.selftext or "")
            content.append(text)
    except Exception as e:
        print(f"Error fetching data for u/{username}: {e}")

    return content

def analyze_content(texts):
    sentiment_analyzer = pipeline("sentiment-analysis")
    classifier = pipeline("zero-shot-classification")

    joined_text = " ".join(texts[:30])[:2000]
    topics = [
        "technology", "politics", "sports", "relationships", "finance",
        "gaming", "science", "health", "spirituality", "memes", "movies", "education"
    ]

    sentiment = sentiment_analyzer(joined_text[:1000])
    topic_result = classifier(joined_text, candidate_labels=topics)

    return {
        "sentiment": sentiment,
        "top_interests": topic_result["labels"][:3],
        "confidence_scores": topic_result["scores"][:3]
    }

def estimate_personality(texts):
    joined = " ".join(texts[:50]).lower()

    return {
        "Introvert": 70 if joined.count(" i ") > joined.count(" we ") else 30,
        "Extrovert": 30 if joined.count(" i ") > joined.count(" we ") else 70,
        "Thinking": 70 if "logic" in joined or "fact" in joined else 30,
        "Feeling": 70 if "feel" in joined or "emotion" in joined else 30,
        "Judging": 70 if "should" in joined or "must" in joined else 30,
        "Perceiving": 70 if "maybe" in joined or "could" in joined else 30
    }

def classify_traits(text):
    classifier = pipeline("zero-shot-classification")
    traits = ["Humorous", "Helpful", "Sarcastic", "Curious", "Supportive", "Logical", "Argumentative"]
    result = classifier(text, candidate_labels=traits)
    return result["labels"][:4]

def generate_persona(username, texts, analysis, personality, traits):
    return {
        "name": f"u/{username}",
        "tier": "Active Redditor",
        "archetype": "The Explorer" if "Curious" in traits else "The Analyst",
        "traits": traits,
        "motivations": {
            interest: round(score * 100)
            for interest, score in zip(analysis["top_interests"], analysis["confidence_scores"])
        },
        "personality": personality,
        "habits": [
            "Posts frequently in niche subreddits.",
            "Engages with others through comments.",
            "Mixes opinion with helpfulness."
        ],
        "frustrations": [
            "Low-effort content or spam.",
            "Overgeneralized takes or poor reasoning."
        ],
        "goals": [
            "To connect with thoughtful users.",
            "To explore new ideas and perspectives."
        ],
        "quote": texts[0][:100] + "...",
        "sentiment": analysis["sentiment"][0]["label"]
    }

def render_persona_html(persona, image_path="k.png"):
    html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Persona - {persona['name']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        h1 {{ color: #d25a00; }}
        .profile-pic {{ float: right; max-height: 150px; margin-left: 20px; border-radius: 12px; }}
        .traits span {{ background: #eee; padding: 5px 10px; margin: 5px; border-radius: 5px; display: inline-block; }}
        ul {{ margin-left: 0; padding-left: 1em; }}
        li {{ margin-bottom: 8px; }}
        blockquote {{ font-style: italic; color: #555; }}
    </style>
</head>
<body>
    <img src="{image_path}" class="profile-pic" alt="Profile Picture">
    <h1>{persona['name']}</h1>
    <p><strong>Tier:</strong> {persona['tier']}<br>
       <strong>Archetype:</strong> {persona['archetype']}<br>
       <strong>Sentiment:</strong> {persona['sentiment']}</p>

    <h2>Traits</h2>
    <div class="traits">
        {''.join(f"<span>{trait}</span>" for trait in persona['traits'])}
    </div>

    <h2>Motivations</h2>
    <ul>
        {''.join(f"<li><strong>{k}:</strong> {v}/100</li>" for k, v in persona['motivations'].items())}
    </ul>

    <h2>Personality</h2>
    <ul>
        {''.join(f"<li><strong>{k}:</strong> {v}/100</li>" for k, v in persona['personality'].items())}
    </ul>

    <h2>Habits</h2>
    <ul>
        {''.join(f"<li>{h}</li>" for h in persona['habits'])}
    </ul>

    <h2>Frustrations</h2>
    <ul>
        {''.join(f"<li>{f}</li>" for f in persona['frustrations'])}
    </ul>

    <h2>Goals</h2>
    <ul>
        {''.join(f"<li>{g}</li>" for g in persona['goals'])}
    </ul>

    <blockquote>"{persona['quote']}"</blockquote>
</body>
</html>
"""
    return html_template

def create_persona_for_hungry():
    profile_url = "https://www.reddit.com/user/Hungry-Move-6603/"
    username = extract_username(profile_url)
    print(f"üîç Generating persona for u/{username}...")

    texts = fetch_user_content(username)
    if not texts:
        print("‚ö†Ô∏è No content found.")
        return

    analysis = analyze_content(texts)
    personality = estimate_personality(texts)
    traits = classify_traits(" ".join(texts[:10]))

    persona = generate_persona(username, texts, analysis, personality, traits)
    html = render_persona_html(persona, image_path="k.png")

    file_path = f"persona_{username}.html"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html)

    print(f"‚úÖ Persona HTML saved to {file_path}")

# ‚úÖ Run it
if __name__ == "__main__":
    create_persona_for_hungry()